# -*- coding: utf-8 -*-
"""langchain agent ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18HGsAHtcDqrNyx3IsJgGjHkAW8CYNhnW
"""

# Install necessary packages
!pip install langchain google-generativeai

from langchain.chains import ConversationChain
from langchain.memory import ConversationBufferMemory
from google.generativeai import GenerativeModel, configure
import os

# Set your Gemini API key securely
api_key = "AIzaSyA4hVy6AEEemzU1dq1vPyBpVn9Tx_JzZqc"
configure(api_key=api_key)  # Configure the API key for google-generativeai

# Initialize the Gemini model using the google-generativeai library
gemini_model = GenerativeModel("gemini-pro")

# Create a custom wrapper for Gemini to make it compatible with LangChain
from langchain.llms.base import LLM
from typing import Optional, List, Mapping, Any

class GeminiLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "gemini"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        response = gemini_model.generate_content(prompt)
        return response.text

# Initialize Gemini LLM
llm = GeminiLLM()

# Set up memory for conversation context
memory = ConversationBufferMemory()

# Create a conversation chain with memory
conversation = ConversationChain(llm=llm, memory=memory, verbose=True)

chat_model = ChatGoogleGenerativeAI(model="gemini-pro", api_key=os.environ["GOOGLE_API_KEY"])

# Define the agent logic for continuous chat
def chat_with_api():
    print("Chat session started! Type 'exit' to end the chat.\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['exit', 'quit']:
            print("Chat session ended. Goodbye!")
            break
        response = chat_model.invoke(user_input)
        print(f"AI: {response.content}")

# Start the chat
if __name__ == "__main__":
    chat_with_api()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
if not GOOGLE_API_KEY:
    GOOGLE_API_KEY = getpass("Enter your Google API Key: ")
    os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY

# Initialize the chat model
chat_model = ChatGoogleGenerativeAI(model="gemini-pro", api_key=os.environ["GOOGLE_API_KEY"])

# Define the agent logic for continuous chat
def chat_with_api():
    print("Chat session started! Type 'exit' to end the chat.\n")
    while True:
        user_input = input("You: ")
        if user_input.lower() in ['exit', 'quit']:
            print("Chat session ended. Goodbye!")
            break
        response = chat_model.invoke(user_input)
        print(f"AI: {response.content}")

# Start the chat
if __name__ == "__main__":
    chat_with_api()

